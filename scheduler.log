nohup: ignoring input
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-09-06T00:50:50.043-0500[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-09-06T00:50:50.044-0500[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2025-09-06 00:50:50 -0500] [8744] [INFO] Starting gunicorn 22.0.0
[2025-09-06 00:50:50 -0500] [8744] [INFO] Listening at: http://[::]:8793 (8744)
[2025-09-06 00:50:50 -0500] [8744] [INFO] Using worker: sync
[[34m2025-09-06T00:50:50.068-0500[0m] {[34mscheduler_job_runner.py:[0m799} INFO[0m - Starting the scheduler[0m
[2025-09-06 00:50:50 -0500] [8745] [INFO] Booting worker with pid: 8745
[[34m2025-09-06T00:50:50.068-0500[0m] {[34mscheduler_job_runner.py:[0m806} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-09-06T00:50:50.072-0500[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 8746[0m
[[34m2025-09-06T00:50:50.072-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T00:50:50.074-0500[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-09-06T00:50:50.088-0500] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-09-06T00:50:50.091-0500[0m] {[34mscheduler_job_runner.py:[0m1621} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-09-06 00:50:50 -0500] [8747] [INFO] Booting worker with pid: 8747
[[34m2025-09-06T00:50:50.884-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:50:00+00:00, run_after=2025-09-06 05:55:00+00:00[0m
[[34m2025-09-06T00:50:50.935-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:45:00+00:00 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-05T01:10:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:50:50.935-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:50:50.935-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T00:50:50.936-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:45:00+00:00 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-05T01:10:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:50:50.937-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T00:50:50.937-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:50:50.937-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-05T01:10:00+00:00', try_number=2, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:50:50.937-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-05T01:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:50:50.942-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:50:51.650-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:50:52.024-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:52.085-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:52.169-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:52.200-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:52.263-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:50:52.263-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:52.287-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:45:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:50:52.797-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-05T01:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:50:53.426-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:50:53.770-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:53.811-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:53.891-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:53.918-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:53.978-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:50:53.978-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:53.998-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-05T01:10:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:50:54.713-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T00:50:54.714-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-05T01:10:00+00:00', try_number=2, map_index=2)[0m
[[34m2025-09-06T00:50:54.718-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-05T01:10:00+00:00, map_index=2, run_start_date=2025-09-06 05:50:54.036116+00:00, run_end_date=2025-09-06 05:50:54.306189+00:00, run_duration=0.270073, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=1734, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:50:50.936398+00:00, queued_by_job_id=1732, pid=8782[0m
[[34m2025-09-06T00:50:54.718-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T05:45:00+00:00, map_index=-1, run_start_date=2025-09-06 05:50:52.331582+00:00, run_end_date=2025-09-06 05:50:52.415325+00:00, run_duration=0.083743, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1733, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:50:50.936398+00:00, queued_by_job_id=1732, pid=8772[0m
[[34m2025-09-06T00:50:54.851-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun market_data_pipeline @ 2025-09-05 01:10:00+00:00: scheduled__2025-09-05T01:10:00+00:00, state:running, queued_at: 2025-09-05 01:15:01.716789+00:00. externally triggered: False> failed[0m
[[34m2025-09-06T00:50:54.852-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-05 01:10:00+00:00, run_id=scheduled__2025-09-05T01:10:00+00:00, run_start_date=2025-09-05 01:15:01.740651+00:00, run_end_date=2025-09-06 05:50:54.852296+00:00, run_duration=102953.111645, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-05 01:10:00+00:00, data_interval_end=2025-09-05 01:15:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T00:50:54.856-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:45:00+00:00, run_after=2025-09-06 05:50:00+00:00[0m
[[34m2025-09-06T00:50:54.865-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T00:50:54.866-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:50:54.866-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T00:50:54.867-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T00:50:54.867-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:50:54.871-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:50:55.596-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:50:55.985-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:56.032-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:56.123-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:56.151-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:56.213-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:50:56.214-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:56.239-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:45:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:50:56.756-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T00:50:56.760-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T05:45:00+00:00, map_index=-1, run_start_date=2025-09-06 05:50:56.271629+00:00, run_end_date=2025-09-06 05:50:56.353695+00:00, run_duration=0.082066, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1735, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:50:54.866676+00:00, queued_by_job_id=1732, pid=8796[0m
[[34m2025-09-06T00:50:57.201-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:50:00+00:00, run_after=2025-09-06 05:55:00+00:00[0m
[[34m2025-09-06T00:50:57.226-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:50:57.227-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:50:57.227-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T00:50:57.227-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T00:50:57.227-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:50:57.229-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:50:57.229-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T00:50:57.229-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:50:57.229-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:50:57.230-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:50:57.230-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:50:57.234-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T00:50:58.128-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:50:58.551-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:58.611-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:58.717-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:58.753-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:50:58.823-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:50:58.823-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:50:58.845-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:50:59.600-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:51:00.403-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:51:00.798-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:00.849-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:00.967-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:01.007-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:01.108-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:51:01.108-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:01.139-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:51:01.994-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:51:02.783-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:51:03.196-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:03.248-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:03.341-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:03.372-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:03.440-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:51:03.440-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:03.461-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:51:04.227-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T00:51:04.227-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T00:51:04.227-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T00:51:04.231-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:45:00+00:00, map_index=0, run_start_date=2025-09-06 05:50:58.893153+00:00, run_end_date=2025-09-06 05:50:59.152037+00:00, run_duration=0.258884, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1736, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:50:57.228072+00:00, queued_by_job_id=1732, pid=8818[0m
[[34m2025-09-06T00:51:04.231-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:45:00+00:00, map_index=1, run_start_date=2025-09-06 05:51:01.191749+00:00, run_end_date=2025-09-06 05:51:01.501660+00:00, run_duration=0.309911, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1737, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:50:57.228072+00:00, queued_by_job_id=1732, pid=8831[0m
[[34m2025-09-06T00:51:04.231-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:45:00+00:00, map_index=2, run_start_date=2025-09-06 05:51:03.502508+00:00, run_end_date=2025-09-06 05:51:03.769437+00:00, run_duration=0.266929, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=1738, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:50:57.228072+00:00, queued_by_job_id=1732, pid=8844[0m
[[34m2025-09-06T00:51:29.439-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:51:29.440-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:51:29.440-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T00:51:29.440-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T00:51:29.440-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:51:29.441-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=3, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:51:29.441-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T00:51:29.441-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=3, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:51:29.441-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:51:29.441-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=4, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:51:29.442-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:51:29.446-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T00:51:30.280-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:51:30.686-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:30.733-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:30.824-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:30.855-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:30.925-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:51:30.925-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:30.948-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:51:31.768-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:51:32.743-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:51:33.210-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:33.270-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:33.405-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:33.463-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:33.582-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:51:33.582-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:33.615-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:51:34.462-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:51:35.468-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:51:35.999-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:36.071-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:36.180-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:36.226-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:51:36.320-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:51:36.321-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:51:36.349-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:51:37.148-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=3, map_index=0)[0m
[[34m2025-09-06T00:51:37.148-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=3, map_index=1)[0m
[[34m2025-09-06T00:51:37.148-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=4, map_index=2)[0m
[[34m2025-09-06T00:51:37.151-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-04T07:00:00+00:00, map_index=0, run_start_date=2025-09-06 05:51:30.988107+00:00, run_end_date=2025-09-06 05:51:31.273229+00:00, run_duration=0.285122, state=success, executor_state=success, try_number=3, max_tries=3, job_id=1739, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:51:29.440520+00:00, queued_by_job_id=1732, pid=8893[0m
[[34m2025-09-06T00:51:37.151-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-04T07:00:00+00:00, map_index=1, run_start_date=2025-09-06 05:51:33.673853+00:00, run_end_date=2025-09-06 05:51:34.018997+00:00, run_duration=0.345144, state=success, executor_state=success, try_number=3, max_tries=3, job_id=1740, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:51:29.440520+00:00, queued_by_job_id=1732, pid=8906[0m
[[34m2025-09-06T00:51:37.151-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-04T07:00:00+00:00, map_index=2, run_start_date=2025-09-06 05:51:36.407157+00:00, run_end_date=2025-09-06 05:51:36.701891+00:00, run_duration=0.294734, state=up_for_retry, executor_state=success, try_number=4, max_tries=4, job_id=1741, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:51:29.440520+00:00, queued_by_job_id=1732, pid=8919[0m
[[34m2025-09-06T00:55:01.763-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:55:00+00:00, run_after=2025-09-06 06:00:00+00:00[0m
[[34m2025-09-06T00:55:01.797-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T00:55:01.797-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:55:01.797-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T00:55:01.798-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T00:55:01.798-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:55:01.801-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:55:02.541-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:55:02.946-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:02.996-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:03.093-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:03.127-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:03.199-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:55:03.199-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:03.225-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:50:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:55:03.778-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T00:55:03.780-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=-1, run_start_date=2025-09-06 05:55:03.263479+00:00, run_end_date=2025-09-06 05:55:03.354632+00:00, run_duration=0.091153, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1742, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:55:01.797828+00:00, queued_by_job_id=1732, pid=9818[0m
[[34m2025-09-06T00:55:03.924-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T00:55:03.924-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:55:03.924-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T00:55:03.925-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T00:55:03.925-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:55:03.928-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T00:55:04.668-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:55:05.062-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:05.110-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:05.202-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:05.234-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:05.315-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:55:05.315-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:05.352-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:50:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:55:05.943-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T00:55:05.946-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=-1, run_start_date=2025-09-06 05:55:05.399451+00:00, run_end_date=2025-09-06 05:55:05.499501+00:00, run_duration=0.10005, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1743, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:55:03.925112+00:00, queued_by_job_id=1732, pid=9829[0m
[[34m2025-09-06T00:55:06.098-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:55:06.098-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:55:06.098-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T00:55:06.098-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T00:55:06.099-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:55:06.099-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:55:06.100-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T00:55:06.100-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:55:06.100-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:55:06.100-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:55:06.100-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:55:06.103-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T00:55:06.844-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:55:07.240-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:07.288-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:07.380-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:07.412-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:07.484-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:55:07.484-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:07.506-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:55:08.267-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:55:09.099-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:55:09.518-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:09.567-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:09.660-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:09.692-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:09.767-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:55:09.767-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:09.788-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:55:10.608-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:55:11.364-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:55:11.765-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:11.817-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:11.913-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:11.946-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:55:12.028-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:55:12.029-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:55:12.053-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:55:12.804-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T00:55:12.804-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T00:55:12.805-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T00:55:12.807-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=0, run_start_date=2025-09-06 05:55:07.548659+00:00, run_end_date=2025-09-06 05:55:07.848387+00:00, run_duration=0.299728, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1744, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:55:06.099285+00:00, queued_by_job_id=1732, pid=9840[0m
[[34m2025-09-06T00:55:12.807-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=1, run_start_date=2025-09-06 05:55:09.831099+00:00, run_end_date=2025-09-06 05:55:10.149599+00:00, run_duration=0.3185, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1745, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:55:06.099285+00:00, queued_by_job_id=1732, pid=9853[0m
[[34m2025-09-06T00:55:12.807-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=2, run_start_date=2025-09-06 05:55:12.094892+00:00, run_end_date=2025-09-06 05:55:12.383393+00:00, run_duration=0.288501, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=1746, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:55:06.099285+00:00, queued_by_job_id=1732, pid=9866[0m
[[34m2025-09-06T00:55:50.266-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T00:56:04.560-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:56:04.560-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:56:04.561-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:56:04.561-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=2, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:56:04.561-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:56:04.565-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:56:05.305-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:56:05.708-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:56:05.759-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:56:05.854-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:56:05.885-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:56:05.957-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:56:05.957-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:56:05.979-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:45:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:56:06.763-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:45:00+00:00', try_number=2, map_index=2)[0m
[[34m2025-09-06T00:56:06.766-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:45:00+00:00, map_index=2, run_start_date=2025-09-06 05:56:06.020940+00:00, run_end_date=2025-09-06 05:56:06.322855+00:00, run_duration=0.301915, state=failed, executor_state=success, try_number=2, max_tries=1, job_id=1747, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:56:04.561272+00:00, queued_by_job_id=1732, pid=9976[0m
[[34m2025-09-06T00:56:06.913-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 05:45:00+00:00: scheduled__2025-09-06T05:45:00+00:00, state:running, queued_at: 2025-09-06 05:50:50.872253+00:00. externally triggered: False> failed[0m
[[34m2025-09-06T00:56:06.913-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 05:45:00+00:00, run_id=scheduled__2025-09-06T05:45:00+00:00, run_start_date=2025-09-06 05:50:50.898066+00:00, run_end_date=2025-09-06 05:56:06.913257+00:00, run_duration=316.015191, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 05:45:00+00:00, data_interval_end=2025-09-06 05:50:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T00:56:06.915-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:50:00+00:00, run_after=2025-09-06 05:55:00+00:00[0m
[[34m2025-09-06T00:56:08.044-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:55:00+00:00, run_after=2025-09-06 06:00:00+00:00[0m
[[34m2025-09-06T00:56:37.610-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:56:37.610-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:56:37.610-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:56:37.611-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=5, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:56:37.612-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:56:37.615-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-04T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:56:38.411-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:56:38.822-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:56:38.874-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:56:38.971-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:56:39.005-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:56:39.080-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:56:39.081-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:56:39.103-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-04T07:00:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T00:56:39.811-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-04T07:00:00+00:00', try_number=5, map_index=2)[0m
[[34m2025-09-06T00:56:39.814-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-04T07:00:00+00:00, map_index=2, run_start_date=2025-09-06 05:56:39.147630+00:00, run_end_date=2025-09-06 05:56:39.390530+00:00, run_duration=0.2429, state=failed, executor_state=success, try_number=5, max_tries=4, job_id=1748, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:56:37.611275+00:00, queued_by_job_id=1732, pid=10035[0m
[[34m2025-09-06T00:56:40.056-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun market_data_pipeline @ 2025-09-04 07:00:00+00:00: scheduled__2025-09-04T07:00:00+00:00, state:running, queued_at: 2025-09-06 05:51:29.330264+00:00. externally triggered: False> failed[0m
[[34m2025-09-06T00:56:40.056-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-04 07:00:00+00:00, run_id=scheduled__2025-09-04T07:00:00+00:00, run_start_date=2025-09-06 05:51:29.408548+00:00, run_end_date=2025-09-06 05:56:40.056395+00:00, run_duration=310.647847, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-04 07:00:00+00:00, data_interval_end=2025-09-04 07:05:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T00:56:40.058-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:50:00+00:00, run_after=2025-09-06 05:55:00+00:00[0m
[[34m2025-09-06T00:57:44.261-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:57:44.262-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T00:57:44.262-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T00:57:44.262-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T00:57:44.263-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:57:44.263-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:57:44.263-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T00:57:44.263-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:57:44.268-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T00:57:45.057-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:57:45.477-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:57:45.527-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:57:45.622-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:57:45.654-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:57:45.724-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:57:45.725-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:57:45.746-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T00:57:46.143-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T00:57:46.883-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T00:57:47.279-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:57:47.328-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:57:47.421-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T00:57:47.454-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T00:57:47.526-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T00:57:47.526-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T00:57:47.547-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [up_for_retry]> on host CK-x1carbon[0m
[[34m2025-09-06T00:57:47.947-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T00:57:47.947-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T00:57:47.950-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=1, run_start_date=None, run_end_date=2025-09-06 05:57:44.346657+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:57:44.262424+00:00, queued_by_job_id=1732, pid=10153[0m
[[34m2025-09-06T00:57:47.950-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=2, run_start_date=None, run_end_date=2025-09-06 05:57:44.554606+00:00, run_duration=None, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 05:57:44.262424+00:00, queued_by_job_id=1732, pid=10153[0m
[[34m2025-09-06T01:00:01.785-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:00:00+00:00, run_after=2025-09-06 06:05:00+00:00[0m
[[34m2025-09-06T01:00:01.824-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:01.824-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:00:01.824-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:01.825-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:00:01.825-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:01.829-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:02.658-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:03.057-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:03.106-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:03.204-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:03.242-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:03.323-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:03.324-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:03.356-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T05:55:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:03.969-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:00:03.973-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T05:55:00+00:00, map_index=-1, run_start_date=2025-09-06 06:00:03.398722+00:00, run_end_date=2025-09-06 06:00:03.491154+00:00, run_duration=0.092432, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1751, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:01.825065+00:00, queued_by_job_id=1732, pid=10416[0m
[[34m2025-09-06T01:00:04.340-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:04.340-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:00:04.340-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:04.342-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:00:04.342-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:04.346-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:05.533-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:06.146-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:06.227-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:06.346-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:06.394-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:06.515-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:06.516-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:06.560-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T05:55:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:07.356-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:00:07.359-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T05:55:00+00:00, map_index=-1, run_start_date=2025-09-06 06:00:06.631687+00:00, run_end_date=2025-09-06 06:00:06.776824+00:00, run_duration=0.145137, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1752, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:04.341364+00:00, queued_by_job_id=1732, pid=10427[0m
[[34m2025-09-06T01:00:07.917-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:00:07.917-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:00:07.917-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:00:07.917-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:00:07.917-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:00:07.918-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:07.919-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:00:07.919-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:07.919-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:00:07.919-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:07.919-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:00:07.923-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:00:09.056-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:09.537-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:09.598-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:09.698-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:09.737-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:09.832-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:09.833-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:09.861-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:10.647-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:00:11.760-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:12.273-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:12.329-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:12.421-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:12.455-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:12.529-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:12.529-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:12.550-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:13.391-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:00:14.444-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:14.949-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:15.011-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:15.117-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:15.161-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:15.263-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:15.264-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:15.303-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:55:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:16.227-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:00:16.227-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:00:16.228-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:00:16.232-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:55:00+00:00, map_index=0, run_start_date=2025-09-06 06:00:09.912017+00:00, run_end_date=2025-09-06 06:00:10.204445+00:00, run_duration=0.292428, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1753, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:07.918126+00:00, queued_by_job_id=1732, pid=10454[0m
[[34m2025-09-06T01:00:16.232-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:55:00+00:00, map_index=1, run_start_date=2025-09-06 06:00:12.593093+00:00, run_end_date=2025-09-06 06:00:12.947816+00:00, run_duration=0.354723, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1754, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:07.918126+00:00, queued_by_job_id=1732, pid=10471[0m
[[34m2025-09-06T01:00:16.233-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:55:00+00:00, map_index=2, run_start_date=2025-09-06 06:00:15.395673+00:00, run_end_date=2025-09-06 06:00:15.747174+00:00, run_duration=0.351501, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1755, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:07.918126+00:00, queued_by_job_id=1732, pid=10485[0m
[[34m2025-09-06T01:00:16.393-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=2 [scheduled]>
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T05:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:16.393-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:00:16.393-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:00:16.393-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=2 [scheduled]>
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T05:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:16.394-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=2, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:16.394-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:00:16.395-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:00:16.395-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:16.399-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:00:17.482-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:17.967-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:18.025-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:18.122-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:18.161-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:18.259-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:18.259-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:18.287-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T05:50:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:19.133-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T05:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:20.141-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:20.605-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:20.663-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:20.765-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:20.805-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:20.889-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:20.889-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:20.919-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T05:55:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:21.500-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=2, map_index=2)[0m
[[34m2025-09-06T01:00:21.501-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T05:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:00:21.504-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T05:55:00+00:00, map_index=-1, run_start_date=2025-09-06 06:00:20.965224+00:00, run_end_date=2025-09-06 06:00:21.061358+00:00, run_duration=0.096134, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1757, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:16.393948+00:00, queued_by_job_id=1732, pid=10513[0m
[[34m2025-09-06T01:00:21.504-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=2, run_start_date=2025-09-06 06:00:18.357012+00:00, run_end_date=2025-09-06 06:00:18.697784+00:00, run_duration=0.340772, state=success, executor_state=success, try_number=2, max_tries=1, job_id=1756, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:16.393948+00:00, queued_by_job_id=1732, pid=10499[0m
[[34m2025-09-06T01:00:21.641-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 05:55:00+00:00: scheduled__2025-09-06T05:55:00+00:00, state:running, queued_at: 2025-09-06 06:00:01.782061+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:00:21.641-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 05:55:00+00:00, run_id=scheduled__2025-09-06T05:55:00+00:00, run_start_date=2025-09-06 06:00:01.794513+00:00, run_end_date=2025-09-06 06:00:21.641663+00:00, run_duration=19.84715, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 05:55:00+00:00, data_interval_end=2025-09-06 06:00:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:00:21.644-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:00:00+00:00, run_after=2025-09-06 06:05:00+00:00[0m
[[34m2025-09-06T01:00:21.664-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T05:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:21.664-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:00:21.664-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T05:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:00:21.665-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:00:21.666-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:21.670-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T05:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:00:22.621-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:23.179-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:23.259-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:23.381-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:23.427-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:23.523-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:23.524-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:23.558-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T05:50:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:24.175-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T05:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:00:24.177-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T05:50:00+00:00, map_index=-1, run_start_date=2025-09-06 06:00:23.609604+00:00, run_end_date=2025-09-06 06:00:23.718188+00:00, run_duration=0.108584, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1758, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:21.665251+00:00, queued_by_job_id=1732, pid=10524[0m
[[34m2025-09-06T01:00:24.306-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 05:50:00+00:00: scheduled__2025-09-06T05:50:00+00:00, state:running, queued_at: 2025-09-06 05:55:01.759749+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:00:24.306-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 05:50:00+00:00, run_id=scheduled__2025-09-06T05:50:00+00:00, run_start_date=2025-09-06 05:55:01.771360+00:00, run_end_date=2025-09-06 06:00:24.306876+00:00, run_duration=322.535516, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 05:50:00+00:00, data_interval_end=2025-09-06 05:55:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:00:24.309-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 05:55:00+00:00, run_after=2025-09-06 06:00:00+00:00[0m
[[34m2025-09-06T01:00:25.446-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:00:00+00:00, run_after=2025-09-06 06:05:00+00:00[0m
[[34m2025-09-06T01:00:49.732-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:00:49.732-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:00:49.733-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:00:49.733-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:00:49.733-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:00:49.734-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:49.734-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:00:49.734-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:49.734-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:00:49.734-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:00:49.734-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:00:49.738-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:00:50.659-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:51.164-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:51.224-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:51.325-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:51.362-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:51.446-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:51.446-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:51.471-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:51.889-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:00:52.746-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:53.277-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:53.342-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:53.447-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:53.489-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:53.578-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:53.578-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:53.604-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:54.024-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:00:54.860-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:00:55.325-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:55.397-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:55.523-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:55.570-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:00:55.685-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:00:55.685-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:00:55.720-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:00:56.152-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:00:56.152-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:00:56.152-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:00:56.155-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=0, run_start_date=None, run_end_date=2025-09-06 06:00:49.960671+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:49.733553+00:00, queued_by_job_id=1732, pid=10569[0m
[[34m2025-09-06T01:00:56.155-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=1, run_start_date=None, run_end_date=2025-09-06 06:00:50.109134+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:49.733553+00:00, queued_by_job_id=1732, pid=10569[0m
[[34m2025-09-06T01:00:56.155-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=2, run_start_date=None, run_end_date=2025-09-06 06:00:50.289901+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:00:49.733553+00:00, queued_by_job_id=1732, pid=10569[0m
[[34m2025-09-06T01:00:56.171-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:02:42.075-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=3 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=4 [scheduled]>[0m
[[34m2025-09-06T01:02:42.076-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:02:42.076-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:02:42.076-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:02:42.076-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 3/16 running and queued tasks[0m
[[34m2025-09-06T01:02:42.076-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 4/16 running and queued tasks[0m
[[34m2025-09-06T01:02:42.076-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=3 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=4 [scheduled]>[0m
[[34m2025-09-06T01:02:42.077-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:02:42.077-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:02:42.077-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:02:42.077-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:02:42.078-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:02:42.078-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:02:42.078-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=3) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:02:42.078-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '3'][0m
[[34m2025-09-06T01:02:42.078-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=4) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:02:42.078-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '4'][0m
[[34m2025-09-06T01:02:42.082-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:02:42.888-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:02:43.319-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:43.371-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:43.467-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:43.501-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:43.574-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:02:43.574-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:43.596-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:02:43.998-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:02:44.765-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:02:45.158-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:45.209-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:45.309-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:45.343-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:45.419-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:02:45.419-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:45.442-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=1 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:02:45.856-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:02:46.599-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:02:46.996-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:47.045-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:47.140-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:47.173-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:47.246-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:02:47.247-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:47.269-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=2 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:02:47.678-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '3'][0m
[[34m2025-09-06T01:02:48.421-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:02:48.820-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:48.872-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:48.969-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:49.002-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:49.074-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:02:49.074-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:49.096-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=3 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:02:49.497-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '4'][0m
[[34m2025-09-06T01:02:50.241-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:02:50.641-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:50.690-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:50.786-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:50.824-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:02:50.896-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:02:50.896-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:02:50.918-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=4 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:02:51.328-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:02:51.328-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:02:51.328-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:02:51.328-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=3)[0m
[[34m2025-09-06T01:02:51.328-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=4)[0m
[[34m2025-09-06T01:02:51.331-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=0, run_start_date=None, run_end_date=2025-09-06 06:02:42.668933+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:02:42.076835+00:00, queued_by_job_id=1732, pid=10804[0m
[[34m2025-09-06T01:02:51.331-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=1, run_start_date=None, run_end_date=2025-09-06 06:02:42.357301+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:02:42.076835+00:00, queued_by_job_id=1732, pid=10804[0m
[[34m2025-09-06T01:02:51.331-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=2, run_start_date=None, run_end_date=2025-09-06 06:02:42.168748+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:02:42.076835+00:00, queued_by_job_id=1732, pid=10804[0m
[[34m2025-09-06T01:02:51.331-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=3, run_start_date=None, run_end_date=2025-09-06 06:02:42.802698+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:02:42.076835+00:00, queued_by_job_id=1732, pid=10804[0m
[[34m2025-09-06T01:02:51.331-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=4, run_start_date=None, run_end_date=2025-09-06 06:02:42.553875+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:02:42.076835+00:00, queued_by_job_id=1732, pid=10804[0m
[[34m2025-09-06T01:05:01.544-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:05:00+00:00, run_after=2025-09-06 06:10:00+00:00[0m
[[34m2025-09-06T01:05:01.577-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:05:01.577-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:05:01.577-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:05:01.578-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:05:01.579-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:05:01.583-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:05:02.646-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:05:03.243-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:03.343-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:03.468-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:03.515-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:03.634-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:05:03.634-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:03.681-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:00:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:05:04.402-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:05:04.405-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:00:00+00:00, map_index=-1, run_start_date=2025-09-06 06:05:03.741783+00:00, run_end_date=2025-09-06 06:05:03.900256+00:00, run_duration=0.158473, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1767, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:05:01.578100+00:00, queued_by_job_id=1732, pid=11095[0m
[[34m2025-09-06T01:05:04.552-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:05:04.552-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:05:04.552-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:05:04.553-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:05:04.553-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:05:04.558-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:05:05.660-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:05:06.159-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:06.219-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:06.318-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:06.356-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:06.439-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:05:06.440-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:06.468-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:00:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:05:07.048-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:05:07.050-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:00:00+00:00, map_index=-1, run_start_date=2025-09-06 06:05:06.511748+00:00, run_end_date=2025-09-06 06:05:06.610616+00:00, run_duration=0.098868, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1768, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:05:04.552927+00:00, queued_by_job_id=1732, pid=11106[0m
[[34m2025-09-06T01:05:07.196-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:05:07.196-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:05:07.196-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:05:07.196-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:05:07.196-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:05:07.198-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:05:07.198-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:05:07.198-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:05:07.198-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:05:07.198-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:05:07.198-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:05:07.203-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:05:08.001-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:05:08.456-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:08.513-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:08.612-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:08.649-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:08.733-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:05:08.733-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:08.758-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:05:09.525-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:05:10.381-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:05:10.842-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:10.899-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:10.999-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:11.036-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:11.117-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:05:11.118-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:11.142-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:05:11.944-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:05:12.720-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:05:13.175-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:13.233-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:13.331-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:13.367-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:13.448-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:05:13.448-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:13.472-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:00:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:05:14.276-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:05:14.276-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:05:14.276-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:05:14.279-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:00:00+00:00, map_index=0, run_start_date=2025-09-06 06:05:08.810226+00:00, run_end_date=2025-09-06 06:05:09.084339+00:00, run_duration=0.274113, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1769, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:05:07.197283+00:00, queued_by_job_id=1732, pid=11117[0m
[[34m2025-09-06T01:05:14.279-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:00:00+00:00, map_index=1, run_start_date=2025-09-06 06:05:11.190898+00:00, run_end_date=2025-09-06 06:05:11.494041+00:00, run_duration=0.303143, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1770, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:05:07.197283+00:00, queued_by_job_id=1732, pid=11130[0m
[[34m2025-09-06T01:05:14.279-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:00:00+00:00, map_index=2, run_start_date=2025-09-06 06:05:13.519660+00:00, run_end_date=2025-09-06 06:05:13.846711+00:00, run_duration=0.327051, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1771, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:05:07.197283+00:00, queued_by_job_id=1732, pid=11143[0m
[[34m2025-09-06T01:05:14.423-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:05:14.423-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:05:14.423-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:05:14.424-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:05:14.424-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:05:14.427-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:05:15.220-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:05:15.672-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:15.729-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:15.828-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:15.865-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:05:15.946-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:05:15.946-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:05:15.975-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:00:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:05:16.641-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:05:16.644-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:00:00+00:00, map_index=-1, run_start_date=2025-09-06 06:05:16.027750+00:00, run_end_date=2025-09-06 06:05:16.133336+00:00, run_duration=0.105586, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1772, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:05:14.423797+00:00, queued_by_job_id=1732, pid=11157[0m
[[34m2025-09-06T01:05:16.775-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:00:00+00:00: scheduled__2025-09-06T06:00:00+00:00, state:running, queued_at: 2025-09-06 06:05:01.539316+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:05:16.776-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:00:00+00:00, run_id=scheduled__2025-09-06T06:00:00+00:00, run_start_date=2025-09-06 06:05:01.555424+00:00, run_end_date=2025-09-06 06:05:16.776182+00:00, run_duration=15.220758, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:00:00+00:00, data_interval_end=2025-09-06 06:05:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:05:16.779-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:05:00+00:00, run_after=2025-09-06 06:10:00+00:00[0m
[[34m2025-09-06T01:05:56.600-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:10:01.879-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:10:00+00:00, run_after=2025-09-06 06:15:00+00:00[0m
[[34m2025-09-06T01:10:01.915-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:10:01.915-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:10:01.915-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:10:01.916-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:10:01.916-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:10:01.920-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:10:02.600-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:10:02.960-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:03.002-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:03.087-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:03.118-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:03.180-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:10:03.180-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:03.206-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:05:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:10:03.736-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:10:03.739-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:05:00+00:00, map_index=-1, run_start_date=2025-09-06 06:10:03.241448+00:00, run_end_date=2025-09-06 06:10:03.327246+00:00, run_duration=0.085798, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1773, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:10:01.915687+00:00, queued_by_job_id=1732, pid=11855[0m
[[34m2025-09-06T01:10:03.982-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:10:03.982-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:10:03.982-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:10:03.983-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:10:03.983-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:10:03.986-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:10:04.637-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:10:04.994-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:05.036-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:05.124-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:05.151-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:05.218-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:10:05.218-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:05.243-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:05:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:10:05.776-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:10:05.779-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:05:00+00:00, map_index=-1, run_start_date=2025-09-06 06:10:05.277488+00:00, run_end_date=2025-09-06 06:10:05.360989+00:00, run_duration=0.083501, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1774, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:10:03.982807+00:00, queued_by_job_id=1732, pid=11867[0m
[[34m2025-09-06T01:10:05.925-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:10:05.925-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:10:05.925-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:10:05.926-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:10:05.926-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:10:05.926-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:10:05.927-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:10:05.927-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:10:05.927-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:10:05.927-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:10:05.927-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:10:05.932-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:10:06.586-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:10:06.952-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:06.995-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:07.082-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:07.114-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:07.178-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:10:07.178-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:07.200-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:10:08.046-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:10:08.696-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:10:09.097-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:09.153-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:09.271-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:09.299-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:09.365-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:10:09.365-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:09.384-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:10:10.139-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:10:10.785-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:10:11.150-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:11.192-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:11.280-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:11.307-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:11.370-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:10:11.370-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:11.389-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:05:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:10:12.202-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:10:12.202-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:10:12.203-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:10:12.205-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:05:00+00:00, map_index=0, run_start_date=2025-09-06 06:10:07.238434+00:00, run_end_date=2025-09-06 06:10:07.627697+00:00, run_duration=0.389263, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1775, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:10:05.926337+00:00, queued_by_job_id=1732, pid=11878[0m
[[34m2025-09-06T01:10:12.205-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:05:00+00:00, map_index=1, run_start_date=2025-09-06 06:10:09.422545+00:00, run_end_date=2025-09-06 06:10:09.736379+00:00, run_duration=0.313834, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1776, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:10:05.926337+00:00, queued_by_job_id=1732, pid=11892[0m
[[34m2025-09-06T01:10:12.205-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:05:00+00:00, map_index=2, run_start_date=2025-09-06 06:10:11.430010+00:00, run_end_date=2025-09-06 06:10:11.779602+00:00, run_duration=0.349592, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1777, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:10:05.926337+00:00, queued_by_job_id=1732, pid=11909[0m
[[34m2025-09-06T01:10:12.359-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:10:12.359-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:10:12.359-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:10:12.360-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:10:12.360-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:10:12.363-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:10:13.037-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:10:13.398-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:13.441-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:13.531-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:13.560-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:10:13.626-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:10:13.626-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:10:13.652-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:05:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:10:14.188-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:10:14.192-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:05:00+00:00, map_index=-1, run_start_date=2025-09-06 06:10:13.686893+00:00, run_end_date=2025-09-06 06:10:13.771253+00:00, run_duration=0.08436, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1778, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:10:12.359545+00:00, queued_by_job_id=1732, pid=11923[0m
[[34m2025-09-06T01:10:14.337-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:05:00+00:00: scheduled__2025-09-06T06:05:00+00:00, state:running, queued_at: 2025-09-06 06:10:01.876750+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:10:14.338-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:05:00+00:00, run_id=scheduled__2025-09-06T06:05:00+00:00, run_start_date=2025-09-06 06:10:01.894252+00:00, run_end_date=2025-09-06 06:10:14.338355+00:00, run_duration=12.444103, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:05:00+00:00, data_interval_end=2025-09-06 06:10:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:10:14.344-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:10:00+00:00, run_after=2025-09-06 06:15:00+00:00[0m
[[34m2025-09-06T01:10:56.807-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:13:13.474-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db manual__2025-09-06T06:13:12.166813+00:00 [scheduled]>[0m
[[34m2025-09-06T01:13:13.474-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:13:13.474-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db manual__2025-09-06T06:13:12.166813+00:00 [scheduled]>[0m
[[34m2025-09-06T01:13:13.475-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:13:13.475-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:13:13.480-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:13:14.270-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:13:14.663-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:14.712-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:14.813-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:14.852-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:14.930-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:13:14.931-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:14.957-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db manual__2025-09-06T06:13:12.166813+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:13:15.527-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:13:15.530-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=-1, run_start_date=2025-09-06 06:13:14.997951+00:00, run_end_date=2025-09-06 06:13:15.096620+00:00, run_duration=0.098669, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1779, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:13:13.474738+00:00, queued_by_job_id=1732, pid=12235[0m
[[34m2025-09-06T01:13:15.678-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers manual__2025-09-06T06:13:12.166813+00:00 [scheduled]>[0m
[[34m2025-09-06T01:13:15.678-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:13:15.678-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers manual__2025-09-06T06:13:12.166813+00:00 [scheduled]>[0m
[[34m2025-09-06T01:13:15.679-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:13:15.679-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:13:15.684-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:13:16.428-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:13:16.818-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:16.867-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:16.961-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:16.994-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:17.066-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:13:17.067-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:17.092-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers manual__2025-09-06T06:13:12.166813+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:13:17.665-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:13:17.668-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=-1, run_start_date=2025-09-06 06:13:17.128861+00:00, run_end_date=2025-09-06 06:13:17.221954+00:00, run_duration=0.093093, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1780, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:13:15.679239+00:00, queued_by_job_id=1732, pid=12246[0m
[[34m2025-09-06T01:13:17.805-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:13:17.805-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:13:17.805-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:13:17.806-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:13:17.806-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:13:17.806-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:13:17.807-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:13:17.807-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:13:17.807-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:13:17.807-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:13:17.807-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:13:17.812-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:13:18.559-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:13:18.981-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:19.031-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:19.127-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:19.159-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:19.233-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:13:19.233-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:19.254-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:13:20.140-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:13:20.905-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:13:21.353-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:21.405-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:21.504-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:21.538-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:21.613-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:13:21.613-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:21.635-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:13:22.513-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:13:23.271-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:13:23.689-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:23.747-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:23.845-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:23.877-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:13:23.949-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:13:23.949-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:13:23.969-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:13:24.720-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:13:24.720-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:13:24.720-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:13:24.723-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=0, run_start_date=2025-09-06 06:13:19.295832+00:00, run_end_date=2025-09-06 06:13:19.666594+00:00, run_duration=0.370762, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=1781, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:13:17.806352+00:00, queued_by_job_id=1732, pid=12258[0m
[[34m2025-09-06T01:13:24.723-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=1, run_start_date=2025-09-06 06:13:21.677226+00:00, run_end_date=2025-09-06 06:13:22.027704+00:00, run_duration=0.350478, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=1782, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:13:17.806352+00:00, queued_by_job_id=1732, pid=12272[0m
[[34m2025-09-06T01:13:24.723-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=2, run_start_date=2025-09-06 06:13:24.010325+00:00, run_end_date=2025-09-06 06:13:24.288960+00:00, run_duration=0.278635, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=1783, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:13:17.806352+00:00, queued_by_job_id=1732, pid=12286[0m
[[34m2025-09-06T01:15:02.084-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:15:00+00:00, run_after=2025-09-06 06:20:00+00:00[0m
[[34m2025-09-06T01:15:02.114-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:10:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:02.114-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:15:02.114-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:10:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:02.115-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:15:02.115-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:02.119-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:02.883-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:03.279-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:03.328-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:03.422-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:03.456-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:03.531-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:03.532-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:03.564-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:10:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:04.158-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:15:04.161-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:10:00+00:00, map_index=-1, run_start_date=2025-09-06 06:15:03.611492+00:00, run_end_date=2025-09-06 06:15:03.713343+00:00, run_duration=0.101851, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1784, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:02.115163+00:00, queued_by_job_id=1732, pid=12462[0m
[[34m2025-09-06T01:15:04.311-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:10:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:04.312-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:15:04.312-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:10:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:04.313-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:15:04.313-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:04.316-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:05.150-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:05.568-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:05.630-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:05.739-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:05.777-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:05.856-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:05.856-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:05.887-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:10:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:06.534-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:15:06.537-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:10:00+00:00, map_index=-1, run_start_date=2025-09-06 06:15:05.928802+00:00, run_end_date=2025-09-06 06:15:06.031957+00:00, run_duration=0.103155, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1785, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:04.312425+00:00, queued_by_job_id=1732, pid=12474[0m
[[34m2025-09-06T01:15:06.779-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:15:06.779-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:15:06.780-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:15:06.780-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:15:06.780-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:15:06.781-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:15:06.781-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:15:06.781-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:15:06.781-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:15:06.781-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:15:06.782-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:15:06.785-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:15:07.627-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:08.026-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:08.076-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:08.168-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:08.200-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:08.274-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:08.274-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:08.295-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:09.054-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:15:09.816-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:10.239-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:10.301-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:10.414-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:10.458-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:10.560-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:10.560-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:10.589-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:11.409-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:15:12.404-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:13.150-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:13.236-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:13.361-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:13.410-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:13.513-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:13.513-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:13.544-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:10:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:14.378-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:15:14.378-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:15:14.378-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:15:14.381-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:10:00+00:00, map_index=0, run_start_date=2025-09-06 06:15:08.337101+00:00, run_end_date=2025-09-06 06:15:08.627933+00:00, run_duration=0.290832, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1786, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:06.780447+00:00, queued_by_job_id=1732, pid=12485[0m
[[34m2025-09-06T01:15:14.381-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:10:00+00:00, map_index=1, run_start_date=2025-09-06 06:15:10.646464+00:00, run_end_date=2025-09-06 06:15:10.974398+00:00, run_duration=0.327934, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1787, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:06.780447+00:00, queued_by_job_id=1732, pid=12498[0m
[[34m2025-09-06T01:15:14.381-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:10:00+00:00, map_index=2, run_start_date=2025-09-06 06:15:13.600690+00:00, run_end_date=2025-09-06 06:15:13.936091+00:00, run_duration=0.335401, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1788, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:06.780447+00:00, queued_by_job_id=1732, pid=12512[0m
[[34m2025-09-06T01:15:14.537-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:10:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:14.537-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:15:14.537-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:10:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:14.538-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:15:14.538-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:14.542-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:15.465-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:16.031-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:16.099-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:16.204-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:16.247-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:16.341-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:16.341-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:16.373-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:10:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:16.918-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:15:16.921-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:10:00+00:00, map_index=-1, run_start_date=2025-09-06 06:15:16.421028+00:00, run_end_date=2025-09-06 06:15:16.522563+00:00, run_duration=0.101535, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1789, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:14.537929+00:00, queued_by_job_id=1732, pid=12526[0m
[[34m2025-09-06T01:15:17.046-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:10:00+00:00: scheduled__2025-09-06T06:10:00+00:00, state:running, queued_at: 2025-09-06 06:15:02.081612+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:15:17.046-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:10:00+00:00, run_id=scheduled__2025-09-06T06:10:00+00:00, run_start_date=2025-09-06 06:15:02.092253+00:00, run_end_date=2025-09-06 06:15:17.046770+00:00, run_duration=14.954517, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:10:00+00:00, data_interval_end=2025-09-06 06:15:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:15:17.049-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:15:00+00:00, run_after=2025-09-06 06:20:00+00:00[0m
[[34m2025-09-06T01:15:25.834-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:15:25.834-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:15:25.834-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:15:25.834-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:15:25.835-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:15:25.836-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=2, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:15:25.836-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:15:25.836-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=2, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:15:25.836-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:15:25.836-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=2, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:15:25.837-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:15:25.840-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:15:26.714-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:27.244-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:27.334-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:27.495-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:27.548-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:27.651-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:27.651-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:27.679-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:28.521-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:15:29.431-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:29.907-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:29.966-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:30.066-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:30.104-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:30.186-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:30.187-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:30.210-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:30.933-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:15:31.743-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:32.258-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:32.324-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:32.430-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:32.475-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:32.564-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:32.564-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:32.588-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T06:13:12.166813+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:33.359-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=2, map_index=0)[0m
[[34m2025-09-06T01:15:33.359-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=2, map_index=1)[0m
[[34m2025-09-06T01:15:33.359-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=2, map_index=2)[0m
[[34m2025-09-06T01:15:33.361-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=0, run_start_date=2025-09-06 06:15:27.730703+00:00, run_end_date=2025-09-06 06:15:28.076927+00:00, run_duration=0.346224, state=success, executor_state=success, try_number=2, max_tries=2, job_id=1790, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:25.835397+00:00, queued_by_job_id=1732, pid=12560[0m
[[34m2025-09-06T01:15:33.361-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=1, run_start_date=2025-09-06 06:15:30.255292+00:00, run_end_date=2025-09-06 06:15:30.509054+00:00, run_duration=0.253762, state=success, executor_state=success, try_number=2, max_tries=2, job_id=1791, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:25.835397+00:00, queued_by_job_id=1732, pid=12574[0m
[[34m2025-09-06T01:15:33.361-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=2, run_start_date=2025-09-06 06:15:32.637458+00:00, run_end_date=2025-09-06 06:15:32.913622+00:00, run_duration=0.276164, state=success, executor_state=success, try_number=2, max_tries=2, job_id=1792, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:25.835397+00:00, queued_by_job_id=1732, pid=12588[0m
[[34m2025-09-06T01:15:33.508-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts manual__2025-09-06T06:13:12.166813+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:33.509-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:15:33.509-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts manual__2025-09-06T06:13:12.166813+00:00 [scheduled]>[0m
[[34m2025-09-06T01:15:33.510-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:15:33.510-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:33.514-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'manual__2025-09-06T06:13:12.166813+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:15:34.321-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:15:34.775-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:34.833-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:34.934-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:34.972-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:15:35.057-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:15:35.058-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:15:35.087-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts manual__2025-09-06T06:13:12.166813+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:15:35.692-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='manual__2025-09-06T06:13:12.166813+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:15:35.695-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=manual__2025-09-06T06:13:12.166813+00:00, map_index=-1, run_start_date=2025-09-06 06:15:35.135279+00:00, run_end_date=2025-09-06 06:15:35.240368+00:00, run_duration=0.105089, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1793, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:15:33.509610+00:00, queued_by_job_id=1732, pid=12673[0m
[[34m2025-09-06T01:15:35.823-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:13:12.166813+00:00: manual__2025-09-06T06:13:12.166813+00:00, state:running, queued_at: 2025-09-06 06:13:12.176706+00:00. externally triggered: True> successful[0m
[[34m2025-09-06T01:15:35.823-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:13:12.166813+00:00, run_id=manual__2025-09-06T06:13:12.166813+00:00, run_start_date=2025-09-06 06:13:13.452080+00:00, run_end_date=2025-09-06 06:15:35.823657+00:00, run_duration=142.371577, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-09-06 06:05:00+00:00, data_interval_end=2025-09-06 06:10:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:15:56.935-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:20:02.108-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:20:00+00:00, run_after=2025-09-06 06:25:00+00:00[0m
[[34m2025-09-06T01:20:02.133-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:15:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:20:02.134-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:20:02.134-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:15:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:20:02.135-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:20:02.135-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:20:02.139-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:20:02.871-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:20:03.266-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:03.314-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:03.408-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:03.440-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:03.511-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:20:03.512-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:03.537-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:15:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:20:04.105-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:20:04.107-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:15:00+00:00, map_index=-1, run_start_date=2025-09-06 06:20:03.576780+00:00, run_end_date=2025-09-06 06:20:03.666071+00:00, run_duration=0.089291, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1794, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:20:02.134480+00:00, queued_by_job_id=1732, pid=13119[0m
[[34m2025-09-06T01:20:04.253-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:15:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:20:04.254-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:20:04.254-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:15:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:20:04.255-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:20:04.255-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:20:04.258-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:20:05.008-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:20:05.414-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:05.465-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:05.560-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:05.592-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:05.662-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:20:05.662-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:05.689-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:15:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:20:06.258-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:20:06.261-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:15:00+00:00, map_index=-1, run_start_date=2025-09-06 06:20:05.727088+00:00, run_end_date=2025-09-06 06:20:05.822490+00:00, run_duration=0.095402, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1795, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:20:04.254510+00:00, queued_by_job_id=1732, pid=13133[0m
[[34m2025-09-06T01:20:06.407-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:20:06.407-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:20:06.407-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:20:06.407-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:20:06.408-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:20:06.408-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:20:06.408-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:20:06.409-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:20:06.409-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:20:06.409-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:20:06.409-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:20:06.412-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:20:07.134-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:20:07.528-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:07.578-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:07.674-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:07.707-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:07.780-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:20:07.780-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:07.802-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:20:08.550-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:20:09.329-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:20:09.747-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:09.796-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:09.892-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:09.925-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:09.997-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:20:09.997-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:10.019-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:20:10.743-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:20:11.477-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:20:11.874-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:11.924-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:12.020-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:12.052-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:12.121-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:20:12.122-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:12.143-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:15:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:20:12.902-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:20:12.902-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:20:12.902-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:20:12.905-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:15:00+00:00, map_index=0, run_start_date=2025-09-06 06:20:07.844049+00:00, run_end_date=2025-09-06 06:20:08.113638+00:00, run_duration=0.269589, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1796, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:20:06.408227+00:00, queued_by_job_id=1732, pid=13144[0m
[[34m2025-09-06T01:20:12.905-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:15:00+00:00, map_index=1, run_start_date=2025-09-06 06:20:10.062788+00:00, run_end_date=2025-09-06 06:20:10.318600+00:00, run_duration=0.255812, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1797, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:20:06.408227+00:00, queued_by_job_id=1732, pid=13157[0m
[[34m2025-09-06T01:20:12.905-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:15:00+00:00, map_index=2, run_start_date=2025-09-06 06:20:12.185134+00:00, run_end_date=2025-09-06 06:20:12.461988+00:00, run_duration=0.276854, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1798, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:20:06.408227+00:00, queued_by_job_id=1732, pid=13170[0m
[[34m2025-09-06T01:20:13.048-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:15:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:20:13.048-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:20:13.049-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:15:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:20:13.049-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:20:13.049-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:20:13.053-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:20:13.799-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:20:14.211-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:14.261-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:14.356-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:14.388-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:20:14.459-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:20:14.459-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:20:14.487-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:15:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:20:15.043-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:15:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:20:15.045-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:15:00+00:00, map_index=-1, run_start_date=2025-09-06 06:20:14.530659+00:00, run_end_date=2025-09-06 06:20:14.619099+00:00, run_duration=0.08844, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1799, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:20:13.049247+00:00, queued_by_job_id=1732, pid=13184[0m
[[34m2025-09-06T01:20:15.171-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:15:00+00:00: scheduled__2025-09-06T06:15:00+00:00, state:running, queued_at: 2025-09-06 06:20:02.104431+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:20:15.171-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:15:00+00:00, run_id=scheduled__2025-09-06T06:15:00+00:00, run_start_date=2025-09-06 06:20:02.117168+00:00, run_end_date=2025-09-06 06:20:15.171337+00:00, run_duration=13.054169, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:15:00+00:00, data_interval_end=2025-09-06 06:20:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:20:15.173-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:20:00+00:00, run_after=2025-09-06 06:25:00+00:00[0m
[[34m2025-09-06T01:20:57.066-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:25:02.089-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:25:00+00:00, run_after=2025-09-06 06:30:00+00:00[0m
[[34m2025-09-06T01:25:02.120-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:20:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:25:02.120-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:25:02.120-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:20:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:25:02.121-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:25:02.121-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:25:02.125-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:25:02.779-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:25:03.135-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:03.178-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:03.264-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:03.292-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:03.355-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:25:03.355-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:03.379-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:20:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:25:03.892-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:25:03.894-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:20:00+00:00, map_index=-1, run_start_date=2025-09-06 06:25:03.412555+00:00, run_end_date=2025-09-06 06:25:03.497444+00:00, run_duration=0.084889, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1800, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:25:02.120892+00:00, queued_by_job_id=1732, pid=13975[0m
[[34m2025-09-06T01:25:04.045-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:20:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:25:04.046-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:25:04.046-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:20:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:25:04.046-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:25:04.047-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:25:04.050-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:25:04.696-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:25:05.054-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:05.097-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:05.182-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:05.218-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:05.283-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:25:05.283-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:05.306-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:20:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:25:05.836-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:25:05.838-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:20:00+00:00, map_index=-1, run_start_date=2025-09-06 06:25:05.341642+00:00, run_end_date=2025-09-06 06:25:05.425298+00:00, run_duration=0.083656, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1801, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:25:04.046441+00:00, queued_by_job_id=1732, pid=13987[0m
[[34m2025-09-06T01:25:05.989-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:25:05.989-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:25:05.989-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:25:05.989-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:25:05.989-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:25:05.990-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:25:05.990-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:25:05.990-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:25:05.990-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:25:05.991-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:25:05.991-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:25:05.994-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:25:06.639-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:25:06.993-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:07.036-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:07.120-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:07.148-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:07.211-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:25:07.212-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:07.232-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:25:08.073-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:25:08.713-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:25:09.070-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:09.113-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:09.199-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:09.227-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:09.294-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:25:09.295-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:09.313-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:25:10.067-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:25:10.696-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:25:11.049-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:11.091-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:11.175-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:11.202-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:11.265-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:25:11.265-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:11.284-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:20:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:25:12.045-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:25:12.045-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:25:12.045-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:25:12.047-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:20:00+00:00, map_index=0, run_start_date=2025-09-06 06:25:07.273151+00:00, run_end_date=2025-09-06 06:25:07.649996+00:00, run_duration=0.376845, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1802, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:25:05.989917+00:00, queued_by_job_id=1732, pid=13998[0m
[[34m2025-09-06T01:25:12.047-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:20:00+00:00, map_index=1, run_start_date=2025-09-06 06:25:09.350597+00:00, run_end_date=2025-09-06 06:25:09.649481+00:00, run_duration=0.298884, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1803, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:25:05.989917+00:00, queued_by_job_id=1732, pid=14011[0m
[[34m2025-09-06T01:25:12.048-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:20:00+00:00, map_index=2, run_start_date=2025-09-06 06:25:11.322646+00:00, run_end_date=2025-09-06 06:25:11.636772+00:00, run_duration=0.314126, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1804, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:25:05.989917+00:00, queued_by_job_id=1732, pid=14024[0m
[[34m2025-09-06T01:25:12.210-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:20:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:25:12.210-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:25:12.210-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:20:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:25:12.211-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:25:12.211-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:25:12.215-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:25:12.865-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:25:13.218-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:13.261-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:13.349-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:13.377-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:25:13.441-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:25:13.441-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:25:13.464-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:20:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:25:13.971-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:25:13.974-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:20:00+00:00, map_index=-1, run_start_date=2025-09-06 06:25:13.501197+00:00, run_end_date=2025-09-06 06:25:13.581528+00:00, run_duration=0.080331, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1805, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:25:12.211142+00:00, queued_by_job_id=1732, pid=14038[0m
[[34m2025-09-06T01:25:14.115-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:20:00+00:00: scheduled__2025-09-06T06:20:00+00:00, state:running, queued_at: 2025-09-06 06:25:02.081485+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:25:14.115-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:20:00+00:00, run_id=scheduled__2025-09-06T06:20:00+00:00, run_start_date=2025-09-06 06:25:02.100899+00:00, run_end_date=2025-09-06 06:25:14.115846+00:00, run_duration=12.014947, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:20:00+00:00, data_interval_end=2025-09-06 06:25:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:25:14.118-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:25:00+00:00, run_after=2025-09-06 06:30:00+00:00[0m
[[34m2025-09-06T01:25:57.209-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:30:01.217-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:30:00+00:00, run_after=2025-09-06 06:35:00+00:00[0m
[[34m2025-09-06T01:30:01.247-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:25:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:30:01.247-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:30:01.248-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:25:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:30:01.248-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:30:01.248-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:30:01.252-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:30:01.889-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:30:02.242-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:02.285-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:02.370-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:02.398-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:02.461-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:30:02.461-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:02.486-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:25:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:30:03.010-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:30:03.013-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:25:00+00:00, map_index=-1, run_start_date=2025-09-06 06:30:02.519490+00:00, run_end_date=2025-09-06 06:30:02.602829+00:00, run_duration=0.083339, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1806, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:30:01.248221+00:00, queued_by_job_id=1732, pid=14507[0m
[[34m2025-09-06T01:30:03.172-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:25:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:30:03.172-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:30:03.172-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:25:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:30:03.173-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:30:03.173-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:30:03.176-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:30:03.817-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:30:04.202-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:04.245-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:04.329-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:04.357-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:04.420-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:30:04.420-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:04.444-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:25:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:30:04.969-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:30:04.971-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:25:00+00:00, map_index=-1, run_start_date=2025-09-06 06:30:04.478254+00:00, run_end_date=2025-09-06 06:30:04.565040+00:00, run_duration=0.086786, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1807, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:30:03.172618+00:00, queued_by_job_id=1732, pid=14518[0m
[[34m2025-09-06T01:30:05.122-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:30:05.123-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:30:05.123-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:30:05.123-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:30:05.123-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:30:05.124-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:30:05.125-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:30:05.125-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:30:05.125-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:30:05.125-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:30:05.126-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:30:05.131-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:30:05.780-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:30:06.137-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:06.179-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:06.264-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:06.292-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:06.355-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:30:06.356-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:06.374-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:30:07.200-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:30:07.867-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:30:08.255-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:08.302-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:08.393-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:08.423-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:08.490-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:30:08.490-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:08.510-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:30:09.240-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:30:09.953-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:30:10.328-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:10.374-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:10.464-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:10.493-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:10.560-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:30:10.560-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:10.580-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:25:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:30:11.264-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:30:11.264-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:30:11.264-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:30:11.268-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:25:00+00:00, map_index=0, run_start_date=2025-09-06 06:30:06.411622+00:00, run_end_date=2025-09-06 06:30:06.823109+00:00, run_duration=0.411487, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1808, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:30:05.124029+00:00, queued_by_job_id=1732, pid=14530[0m
[[34m2025-09-06T01:30:11.268-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:25:00+00:00, map_index=1, run_start_date=2025-09-06 06:30:08.551506+00:00, run_end_date=2025-09-06 06:30:08.828744+00:00, run_duration=0.277238, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1809, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:30:05.124029+00:00, queued_by_job_id=1732, pid=14544[0m
[[34m2025-09-06T01:30:11.268-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:25:00+00:00, map_index=2, run_start_date=2025-09-06 06:30:10.620044+00:00, run_end_date=2025-09-06 06:30:10.851026+00:00, run_duration=0.230982, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1810, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:30:05.124029+00:00, queued_by_job_id=1732, pid=14561[0m
[[34m2025-09-06T01:30:11.411-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:25:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:30:11.411-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:30:11.411-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:25:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:30:11.412-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:30:11.412-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:30:11.415-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:30:12.095-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:30:12.471-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:12.516-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:12.604-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:12.635-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:30:12.701-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:30:12.701-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:30:12.725-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:25:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:30:13.231-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:30:13.233-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:25:00+00:00, map_index=-1, run_start_date=2025-09-06 06:30:12.762525+00:00, run_end_date=2025-09-06 06:30:12.849543+00:00, run_duration=0.087018, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1811, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:30:11.411614+00:00, queued_by_job_id=1732, pid=14575[0m
[[34m2025-09-06T01:30:13.357-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:25:00+00:00: scheduled__2025-09-06T06:25:00+00:00, state:running, queued_at: 2025-09-06 06:30:01.209020+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:30:13.358-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:25:00+00:00, run_id=scheduled__2025-09-06T06:25:00+00:00, run_start_date=2025-09-06 06:30:01.229648+00:00, run_end_date=2025-09-06 06:30:13.358160+00:00, run_duration=12.128512, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:25:00+00:00, data_interval_end=2025-09-06 06:30:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:30:13.360-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:30:00+00:00, run_after=2025-09-06 06:35:00+00:00[0m
[[34m2025-09-06T01:30:57.672-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:35:01.312-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:35:00+00:00, run_after=2025-09-06 06:40:00+00:00[0m
[[34m2025-09-06T01:35:01.345-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:30:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:35:01.345-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:35:01.345-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:30:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:35:01.346-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:35:01.346-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:35:01.350-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:35:01.988-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:35:02.340-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:02.382-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:02.466-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:02.494-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:02.557-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:35:02.557-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:02.580-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:30:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:35:03.082-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:35:03.084-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:30:00+00:00, map_index=-1, run_start_date=2025-09-06 06:35:02.614793+00:00, run_end_date=2025-09-06 06:35:02.700174+00:00, run_duration=0.085381, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1812, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:35:01.345818+00:00, queued_by_job_id=1732, pid=15043[0m
[[34m2025-09-06T01:35:03.239-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:30:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:35:03.239-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:35:03.239-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:30:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:35:03.240-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:35:03.240-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:35:03.243-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:35:03.879-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:35:04.231-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:04.273-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:04.357-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:04.385-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:04.448-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:35:04.448-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:04.471-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:30:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:35:05.000-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:35:05.002-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:30:00+00:00, map_index=-1, run_start_date=2025-09-06 06:35:04.506628+00:00, run_end_date=2025-09-06 06:35:04.590232+00:00, run_duration=0.083604, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1813, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:35:03.239881+00:00, queued_by_job_id=1732, pid=15055[0m
[[34m2025-09-06T01:35:05.161-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:35:05.161-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:35:05.161-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:35:05.162-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:35:05.162-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:35:05.163-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:35:05.164-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:35:05.164-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:35:05.164-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:35:05.164-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:35:05.165-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:35:05.170-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:35:05.856-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:35:06.211-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:06.254-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:06.337-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:06.364-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:06.428-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:35:06.428-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:06.446-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:35:07.232-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:35:07.862-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:35:08.212-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:08.254-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:08.338-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:08.367-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:08.429-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:35:08.429-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:08.447-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:35:09.149-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:35:09.789-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:35:10.140-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:10.182-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:10.266-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:10.293-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:10.355-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:35:10.356-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:10.374-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:30:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:35:11.160-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:35:11.160-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:35:11.161-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:35:11.163-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:30:00+00:00, map_index=0, run_start_date=2025-09-06 06:35:06.484237+00:00, run_end_date=2025-09-06 06:35:06.835642+00:00, run_duration=0.351405, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1814, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:35:05.162544+00:00, queued_by_job_id=1732, pid=15067[0m
[[34m2025-09-06T01:35:11.163-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:30:00+00:00, map_index=1, run_start_date=2025-09-06 06:35:08.485746+00:00, run_end_date=2025-09-06 06:35:08.758813+00:00, run_duration=0.273067, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1815, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:35:05.162544+00:00, queued_by_job_id=1732, pid=15080[0m
[[34m2025-09-06T01:35:11.163-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:30:00+00:00, map_index=2, run_start_date=2025-09-06 06:35:10.411474+00:00, run_end_date=2025-09-06 06:35:10.775829+00:00, run_duration=0.364355, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1816, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:35:05.162544+00:00, queued_by_job_id=1732, pid=15095[0m
[[34m2025-09-06T01:35:11.328-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:30:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:35:11.328-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:35:11.328-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:30:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:35:11.329-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:35:11.329-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:35:11.332-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:35:11.988-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:35:12.346-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:12.390-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:12.474-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:12.502-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:35:12.566-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:35:12.567-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:35:12.590-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:30:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:35:13.084-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:35:13.087-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:30:00+00:00, map_index=-1, run_start_date=2025-09-06 06:35:12.626433+00:00, run_end_date=2025-09-06 06:35:12.709383+00:00, run_duration=0.08295, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1817, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:35:11.329039+00:00, queued_by_job_id=1732, pid=15109[0m
[[34m2025-09-06T01:35:13.223-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:30:00+00:00: scheduled__2025-09-06T06:30:00+00:00, state:running, queued_at: 2025-09-06 06:35:01.305159+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:35:13.223-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:30:00+00:00, run_id=scheduled__2025-09-06T06:30:00+00:00, run_start_date=2025-09-06 06:35:01.325534+00:00, run_end_date=2025-09-06 06:35:13.223591+00:00, run_duration=11.898057, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:30:00+00:00, data_interval_end=2025-09-06 06:35:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:35:13.225-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:35:00+00:00, run_after=2025-09-06 06:40:00+00:00[0m
[[34m2025-09-06T01:35:57.822-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:40:01.613-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:40:00+00:00, run_after=2025-09-06 06:45:00+00:00[0m
[[34m2025-09-06T01:40:01.639-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:35:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:40:01.640-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:40:01.640-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:35:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:40:01.640-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:40:01.641-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:40:01.644-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:40:02.392-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:40:02.795-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:02.846-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:02.946-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:02.980-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:03.051-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:40:03.052-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:03.077-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:35:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:40:03.637-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:40:03.639-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:35:00+00:00, map_index=-1, run_start_date=2025-09-06 06:40:03.117759+00:00, run_end_date=2025-09-06 06:40:03.210751+00:00, run_duration=0.092992, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1818, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:40:01.640360+00:00, queued_by_job_id=1732, pid=15886[0m
[[34m2025-09-06T01:40:03.777-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:35:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:40:03.777-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:40:03.777-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:35:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:40:03.778-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:40:03.778-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:40:03.781-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:40:04.531-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:40:04.932-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:04.980-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:05.075-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:05.107-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:05.178-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:40:05.178-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:05.203-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:35:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:40:05.781-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:40:05.784-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:35:00+00:00, map_index=-1, run_start_date=2025-09-06 06:40:05.242022+00:00, run_end_date=2025-09-06 06:40:05.336551+00:00, run_duration=0.094529, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1819, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:40:03.777812+00:00, queued_by_job_id=1732, pid=15897[0m
[[34m2025-09-06T01:40:05.930-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:40:05.931-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:40:05.931-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:40:05.931-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:40:05.931-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:40:05.932-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:40:05.932-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:40:05.932-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:40:05.932-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:40:05.932-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:40:05.932-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:40:05.937-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:40:06.677-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:40:07.078-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:07.128-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:07.225-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:07.259-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:07.330-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:40:07.330-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:07.352-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:40:08.158-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:40:08.888-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:40:09.286-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:09.335-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:09.431-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:09.463-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:09.532-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:40:09.533-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:09.554-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:40:10.473-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:40:11.207-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:40:11.644-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:11.696-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:11.795-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:11.829-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:11.906-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:40:11.906-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:11.929-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:35:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:40:12.695-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:40:12.696-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:40:12.696-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:40:12.698-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:35:00+00:00, map_index=0, run_start_date=2025-09-06 06:40:07.397109+00:00, run_end_date=2025-09-06 06:40:07.712949+00:00, run_duration=0.31584, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1820, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:40:05.931517+00:00, queued_by_job_id=1732, pid=15908[0m
[[34m2025-09-06T01:40:12.698-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:35:00+00:00, map_index=1, run_start_date=2025-09-06 06:40:09.595302+00:00, run_end_date=2025-09-06 06:40:10.052390+00:00, run_duration=0.457088, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1821, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:40:05.931517+00:00, queued_by_job_id=1732, pid=15921[0m
[[34m2025-09-06T01:40:12.699-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:35:00+00:00, map_index=2, run_start_date=2025-09-06 06:40:11.973535+00:00, run_end_date=2025-09-06 06:40:12.261953+00:00, run_duration=0.288418, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1822, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:40:05.931517+00:00, queued_by_job_id=1732, pid=15937[0m
[[34m2025-09-06T01:40:13.147-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:35:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:40:13.147-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:40:13.147-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:35:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:40:13.148-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:40:13.148-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:40:13.151-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:40:13.887-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:40:14.287-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:14.338-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:14.434-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:14.466-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:40:14.536-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:40:14.536-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:40:14.562-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:35:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:40:15.112-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:40:15.115-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:35:00+00:00, map_index=-1, run_start_date=2025-09-06 06:40:14.602722+00:00, run_end_date=2025-09-06 06:40:14.693197+00:00, run_duration=0.090475, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1823, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:40:13.148093+00:00, queued_by_job_id=1732, pid=15962[0m
[[34m2025-09-06T01:40:15.241-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:35:00+00:00: scheduled__2025-09-06T06:35:00+00:00, state:running, queued_at: 2025-09-06 06:40:01.609663+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:40:15.241-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:35:00+00:00, run_id=scheduled__2025-09-06T06:35:00+00:00, run_start_date=2025-09-06 06:40:01.621715+00:00, run_end_date=2025-09-06 06:40:15.241240+00:00, run_duration=13.619525, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:35:00+00:00, data_interval_end=2025-09-06 06:40:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:40:15.243-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:40:00+00:00, run_after=2025-09-06 06:45:00+00:00[0m
[[34m2025-09-06T01:40:57.951-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:45:01.325-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:45:00+00:00, run_after=2025-09-06 06:50:00+00:00[0m
[[34m2025-09-06T01:45:01.351-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:40:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:45:01.352-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:45:01.352-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:40:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:45:01.352-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:45:01.353-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:45:01.356-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:45:02.098-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:45:02.492-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:02.541-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:02.636-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:02.670-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:02.743-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:45:02.743-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:02.770-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:40:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:45:03.318-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:45:03.321-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:40:00+00:00, map_index=-1, run_start_date=2025-09-06 06:45:02.808858+00:00, run_end_date=2025-09-06 06:45:02.902513+00:00, run_duration=0.093655, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1824, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:45:01.352433+00:00, queued_by_job_id=1732, pid=16429[0m
[[34m2025-09-06T01:45:03.488-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:40:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:45:03.489-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:45:03.489-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:40:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:45:03.490-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:45:03.490-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:45:03.494-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:45:04.249-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:45:04.644-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:04.694-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:04.788-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:04.820-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:04.891-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:45:04.891-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:04.917-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:40:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:45:05.491-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:45:05.494-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:40:00+00:00, map_index=-1, run_start_date=2025-09-06 06:45:04.957155+00:00, run_end_date=2025-09-06 06:45:05.049008+00:00, run_duration=0.091853, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1825, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:45:03.489589+00:00, queued_by_job_id=1732, pid=16440[0m
[[34m2025-09-06T01:45:05.634-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:45:05.634-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:45:05.634-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:45:05.634-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:45:05.634-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:45:05.635-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:45:05.635-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:45:05.635-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:45:05.635-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:45:05.635-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:45:05.636-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:45:05.639-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:45:06.396-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:45:06.792-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:06.841-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:06.940-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:06.972-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:07.043-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:45:07.043-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:07.064-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:45:07.857-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:45:08.597-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:45:08.993-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:09.043-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:09.141-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:09.174-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:09.245-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:45:09.245-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:09.267-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:45:10.061-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:45:10.820-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:45:11.215-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:11.264-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:11.358-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:11.390-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:11.461-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:45:11.461-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:11.483-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:40:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:45:12.296-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:45:12.296-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:45:12.296-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:45:12.299-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:40:00+00:00, map_index=0, run_start_date=2025-09-06 06:45:07.107143+00:00, run_end_date=2025-09-06 06:45:07.411290+00:00, run_duration=0.304147, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1826, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:45:05.634918+00:00, queued_by_job_id=1732, pid=16451[0m
[[34m2025-09-06T01:45:12.299-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:40:00+00:00, map_index=1, run_start_date=2025-09-06 06:45:09.309771+00:00, run_end_date=2025-09-06 06:45:09.632005+00:00, run_duration=0.322234, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1827, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:45:05.634918+00:00, queued_by_job_id=1732, pid=16464[0m
[[34m2025-09-06T01:45:12.299-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:40:00+00:00, map_index=2, run_start_date=2025-09-06 06:45:11.525848+00:00, run_end_date=2025-09-06 06:45:11.839268+00:00, run_duration=0.31342, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1828, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:45:05.634918+00:00, queued_by_job_id=1732, pid=16477[0m
[[34m2025-09-06T01:45:12.442-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:40:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:45:12.442-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:45:12.443-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:40:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:45:12.443-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:45:12.443-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:45:12.447-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:45:13.199-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:45:13.594-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:13.643-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:13.737-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:13.771-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:45:13.842-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:45:13.843-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:45:13.869-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:40:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:45:14.415-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:45:14.417-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:40:00+00:00, map_index=-1, run_start_date=2025-09-06 06:45:13.912400+00:00, run_end_date=2025-09-06 06:45:14.004514+00:00, run_duration=0.092114, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1829, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:45:12.443249+00:00, queued_by_job_id=1732, pid=16491[0m
[[34m2025-09-06T01:45:14.543-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:40:00+00:00: scheduled__2025-09-06T06:40:00+00:00, state:running, queued_at: 2025-09-06 06:45:01.322354+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:45:14.543-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:40:00+00:00, run_id=scheduled__2025-09-06T06:40:00+00:00, run_start_date=2025-09-06 06:45:01.333821+00:00, run_end_date=2025-09-06 06:45:14.543285+00:00, run_duration=13.209464, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:40:00+00:00, data_interval_end=2025-09-06 06:45:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:45:14.545-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:45:00+00:00, run_after=2025-09-06 06:50:00+00:00[0m
[[34m2025-09-06T01:45:58.490-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:49:00.544-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=3 [scheduled]>[0m
[[34m2025-09-06T01:49:00.544-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:49:00.545-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:49:00.545-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=3 [scheduled]>[0m
[[34m2025-09-06T01:49:00.545-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:49:00.546-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:49:00.546-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=3) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:49:00.546-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '3'][0m
[[34m2025-09-06T01:49:00.549-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:49:01.381-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:49:01.781-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:49:01.831-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:49:01.929-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:49:01.962-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:49:02.035-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:49:02.035-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:49:02.057-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=0 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:49:02.467-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'manual__2025-09-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '3'][0m
[[34m2025-09-06T01:49:03.251-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:49:03.657-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:49:03.707-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:49:03.804-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:49:03.838-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:49:03.916-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:49:03.917-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:49:03.939-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one manual__2025-09-06T00:00:00+00:00 map_index=3 [success]> on host CK-x1carbon[0m
[[34m2025-09-06T01:49:04.355-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:49:04.355-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='manual__2025-09-06T00:00:00+00:00', try_number=1, map_index=3)[0m
[[34m2025-09-06T01:49:04.357-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=0, run_start_date=None, run_end_date=2025-09-06 06:49:00.696196+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:49:00.545366+00:00, queued_by_job_id=1732, pid=16875[0m
[[34m2025-09-06T01:49:04.357-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=manual__2025-09-06T00:00:00+00:00, map_index=3, run_start_date=None, run_end_date=2025-09-06 06:49:00.820216+00:00, run_duration=None, state=success, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:49:00.545366+00:00, queued_by_job_id=1732, pid=16875[0m
[[34m2025-09-06T01:50:01.957-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:50:00+00:00, run_after=2025-09-06 06:55:00+00:00[0m
[[34m2025-09-06T01:50:01.984-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:50:01.984-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:50:01.984-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:50:01.985-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:50:01.985-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:50:01.989-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:50:02.750-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:50:03.120-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:03.168-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:03.282-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:03.315-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:03.382-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:50:03.383-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:03.407-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:45:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:50:03.966-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:50:03.968-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:45:00+00:00, map_index=-1, run_start_date=2025-09-06 06:50:03.444046+00:00, run_end_date=2025-09-06 06:50:03.561974+00:00, run_duration=0.117928, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1832, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:50:01.984826+00:00, queued_by_job_id=1732, pid=17025[0m
[[34m2025-09-06T01:50:04.115-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:50:04.116-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:50:04.116-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:50:04.116-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:50:04.117-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:50:04.120-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:50:04.798-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:50:05.209-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:05.258-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:05.346-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:05.375-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:05.441-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:50:05.441-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:05.466-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:45:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:50:06.044-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:50:06.047-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:45:00+00:00, map_index=-1, run_start_date=2025-09-06 06:50:05.499635+00:00, run_end_date=2025-09-06 06:50:05.584523+00:00, run_duration=0.084888, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1833, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:50:04.116367+00:00, queued_by_job_id=1732, pid=17036[0m
[[34m2025-09-06T01:50:06.212-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:50:06.212-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:50:06.212-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:50:06.212-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:50:06.212-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:50:06.213-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:50:06.213-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:50:06.213-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:50:06.213-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:50:06.213-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:50:06.214-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:50:06.218-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:50:06.940-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:50:07.327-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:07.371-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:07.457-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:07.488-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:07.553-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:50:07.553-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:07.572-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:50:08.364-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:50:09.107-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:50:09.490-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:09.541-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:09.637-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:09.670-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:09.742-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:50:09.743-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:09.764-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:50:10.529-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:50:11.328-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:50:11.723-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:11.780-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:11.882-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:11.911-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:11.977-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:50:11.978-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:11.997-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:45:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:50:12.792-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:50:12.792-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:50:12.792-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:50:12.794-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:45:00+00:00, map_index=0, run_start_date=2025-09-06 06:50:07.611326+00:00, run_end_date=2025-09-06 06:50:07.919562+00:00, run_duration=0.308236, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1834, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:50:06.212919+00:00, queued_by_job_id=1732, pid=17049[0m
[[34m2025-09-06T01:50:12.795-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:45:00+00:00, map_index=1, run_start_date=2025-09-06 06:50:09.802963+00:00, run_end_date=2025-09-06 06:50:10.091406+00:00, run_duration=0.288443, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1835, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:50:06.212919+00:00, queued_by_job_id=1732, pid=17063[0m
[[34m2025-09-06T01:50:12.795-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:45:00+00:00, map_index=2, run_start_date=2025-09-06 06:50:12.036980+00:00, run_end_date=2025-09-06 06:50:12.348387+00:00, run_duration=0.311407, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1836, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:50:06.212919+00:00, queued_by_job_id=1732, pid=17079[0m
[[34m2025-09-06T01:50:13.449-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:50:13.449-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:50:13.449-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:45:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:50:13.450-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:50:13.450-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:50:13.453-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:50:14.237-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:50:14.706-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:14.770-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:14.874-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:14.915-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:50:15.004-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:50:15.004-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:50:15.032-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:45:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:50:15.633-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:50:15.636-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:45:00+00:00, map_index=-1, run_start_date=2025-09-06 06:50:15.076779+00:00, run_end_date=2025-09-06 06:50:15.171619+00:00, run_duration=0.09484, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1837, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:50:13.449701+00:00, queued_by_job_id=1732, pid=17101[0m
[[34m2025-09-06T01:50:15.765-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:45:00+00:00: scheduled__2025-09-06T06:45:00+00:00, state:running, queued_at: 2025-09-06 06:50:01.953778+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:50:15.765-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:45:00+00:00, run_id=scheduled__2025-09-06T06:45:00+00:00, run_start_date=2025-09-06 06:50:01.966282+00:00, run_end_date=2025-09-06 06:50:15.765520+00:00, run_duration=13.799238, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:45:00+00:00, data_interval_end=2025-09-06 06:50:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:50:15.769-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:50:00+00:00, run_after=2025-09-06 06:55:00+00:00[0m
[[34m2025-09-06T01:50:58.625-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T01:55:01.875-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:55:00+00:00, run_after=2025-09-06 07:00:00+00:00[0m
[[34m2025-09-06T01:55:01.907-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:55:01.907-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:55:01.908-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:55:01.908-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T01:55:01.908-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:55:01.913-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:55:02.622-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:55:02.998-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:03.042-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:03.132-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:03.162-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:03.241-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:55:03.241-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:03.273-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:50:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:55:03.848-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:55:03.851-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:50:00+00:00, map_index=-1, run_start_date=2025-09-06 06:55:03.318114+00:00, run_end_date=2025-09-06 06:55:03.424815+00:00, run_duration=0.106701, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1838, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:55:01.908190+00:00, queued_by_job_id=1732, pid=17584[0m
[[34m2025-09-06T01:55:04.303-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:55:04.303-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:55:04.303-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:55:04.304-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T01:55:04.304-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:55:04.309-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:55:04.984-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:55:05.362-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:05.406-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:05.490-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:05.519-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:05.583-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:55:05.584-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:05.607-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:50:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:55:06.139-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:55:06.142-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:50:00+00:00, map_index=-1, run_start_date=2025-09-06 06:55:05.641871+00:00, run_end_date=2025-09-06 06:55:05.726908+00:00, run_duration=0.085037, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1839, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:55:04.304098+00:00, queued_by_job_id=1732, pid=17603[0m
[[34m2025-09-06T01:55:06.294-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:55:06.294-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:55:06.294-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T01:55:06.294-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T01:55:06.294-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T01:55:06.295-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:55:06.295-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:55:06.295-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:55:06.295-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:55:06.296-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T01:55:06.296-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:55:06.299-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T01:55:06.958-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:55:07.322-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:07.365-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:07.450-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:07.478-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:07.541-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:55:07.542-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:07.560-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:55:08.272-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T01:55:08.917-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:55:09.280-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:09.325-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:09.416-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:09.446-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:09.511-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:55:09.511-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:09.531-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:55:10.317-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T01:55:10.971-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:55:11.335-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:11.379-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:11.464-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:11.493-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:11.558-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:55:11.559-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:11.578-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:50:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:55:12.286-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T01:55:12.286-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T01:55:12.286-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T01:55:12.288-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:50:00+00:00, map_index=0, run_start_date=2025-09-06 06:55:07.598254+00:00, run_end_date=2025-09-06 06:55:07.879916+00:00, run_duration=0.281662, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1840, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:55:06.294844+00:00, queued_by_job_id=1732, pid=17614[0m
[[34m2025-09-06T01:55:12.289-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:50:00+00:00, map_index=1, run_start_date=2025-09-06 06:55:09.569884+00:00, run_end_date=2025-09-06 06:55:09.928302+00:00, run_duration=0.358418, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1841, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:55:06.294844+00:00, queued_by_job_id=1732, pid=17627[0m
[[34m2025-09-06T01:55:12.289-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:50:00+00:00, map_index=2, run_start_date=2025-09-06 06:55:11.616966+00:00, run_end_date=2025-09-06 06:55:11.905673+00:00, run_duration=0.288707, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1842, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:55:06.294844+00:00, queued_by_job_id=1732, pid=17642[0m
[[34m2025-09-06T01:55:12.466-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:55:12.466-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T01:55:12.466-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:50:00+00:00 [scheduled]>[0m
[[34m2025-09-06T01:55:12.467-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T01:55:12.467-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:55:12.472-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T01:55:13.294-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T01:55:13.671-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:13.723-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:13.833-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:13.867-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T01:55:13.945-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T01:55:13.945-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T01:55:13.976-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:50:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T01:55:14.649-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T01:55:14.651-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:50:00+00:00, map_index=-1, run_start_date=2025-09-06 06:55:14.019657+00:00, run_end_date=2025-09-06 06:55:14.124311+00:00, run_duration=0.104654, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1843, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 06:55:12.467144+00:00, queued_by_job_id=1732, pid=17663[0m
[[34m2025-09-06T01:55:14.778-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:50:00+00:00: scheduled__2025-09-06T06:50:00+00:00, state:running, queued_at: 2025-09-06 06:55:01.868344+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T01:55:14.778-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:50:00+00:00, run_id=scheduled__2025-09-06T06:50:00+00:00, run_start_date=2025-09-06 06:55:01.886185+00:00, run_end_date=2025-09-06 06:55:14.778875+00:00, run_duration=12.89269, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:50:00+00:00, data_interval_end=2025-09-06 06:55:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T01:55:14.781-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 06:55:00+00:00, run_after=2025-09-06 07:00:00+00:00[0m
[[34m2025-09-06T01:55:58.761-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T02:00:01.587-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 07:00:00+00:00, run_after=2025-09-06 07:05:00+00:00[0m
[[34m2025-09-06T02:00:01.619-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:00:01.620-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:00:01.620-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:00:01.621-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T02:00:01.621-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:00:01.625-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:00:02.349-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:00:02.743-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:02.789-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:02.883-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:02.911-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:02.977-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:00:02.978-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:03.003-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T06:55:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:00:03.536-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:00:03.538-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T06:55:00+00:00, map_index=-1, run_start_date=2025-09-06 07:00:03.039688+00:00, run_end_date=2025-09-06 07:00:03.125861+00:00, run_duration=0.086173, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1844, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:00:01.620534+00:00, queued_by_job_id=1732, pid=18162[0m
[[34m2025-09-06T02:00:03.689-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:00:03.689-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:00:03.690-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:00:03.690-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T02:00:03.690-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:00:03.694-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:00:04.476-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:00:04.892-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:04.942-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:05.031-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:05.059-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:05.130-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:00:05.130-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:05.155-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T06:55:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:00:05.741-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:00:05.743-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T06:55:00+00:00, map_index=-1, run_start_date=2025-09-06 07:00:05.200405+00:00, run_end_date=2025-09-06 07:00:05.304359+00:00, run_duration=0.103954, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1845, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:00:03.690249+00:00, queued_by_job_id=1732, pid=18174[0m
[[34m2025-09-06T02:00:05.890-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T02:00:05.890-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:00:05.890-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T02:00:05.890-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T02:00:05.891-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T02:00:05.891-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:00:05.892-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T02:00:05.892-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:00:05.892-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T02:00:05.892-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:00:05.892-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T02:00:05.897-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T02:00:06.583-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:00:06.943-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:06.986-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:07.071-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:07.099-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:07.163-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:00:07.163-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:07.182-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:00:07.987-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T02:00:08.628-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:00:09.000-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:09.048-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:09.142-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:09.176-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:09.263-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:00:09.263-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:09.293-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:00:10.047-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T02:00:10.704-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:00:11.102-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:11.146-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:11.233-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:11.261-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:11.324-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:00:11.324-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:11.343-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T06:55:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:00:12.111-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T02:00:12.111-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T02:00:12.112-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T02:00:12.114-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:55:00+00:00, map_index=0, run_start_date=2025-09-06 07:00:07.220994+00:00, run_end_date=2025-09-06 07:00:07.574593+00:00, run_duration=0.353599, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1846, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:00:05.891254+00:00, queued_by_job_id=1732, pid=18185[0m
[[34m2025-09-06T02:00:12.114-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:55:00+00:00, map_index=1, run_start_date=2025-09-06 07:00:09.344791+00:00, run_end_date=2025-09-06 07:00:09.610558+00:00, run_duration=0.265767, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1847, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:00:05.891254+00:00, queued_by_job_id=1732, pid=18198[0m
[[34m2025-09-06T02:00:12.114-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T06:55:00+00:00, map_index=2, run_start_date=2025-09-06 07:00:11.381789+00:00, run_end_date=2025-09-06 07:00:11.697968+00:00, run_duration=0.316179, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1848, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:00:05.891254+00:00, queued_by_job_id=1732, pid=18213[0m
[[34m2025-09-06T02:00:12.275-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:00:12.275-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:00:12.275-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:55:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:00:12.276-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T02:00:12.276-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:00:12.280-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T06:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:00:12.922-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:00:13.278-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:13.321-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:13.406-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:13.434-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:00:13.497-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:00:13.497-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:00:13.520-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T06:55:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:00:14.033-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T06:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:00:14.035-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T06:55:00+00:00, map_index=-1, run_start_date=2025-09-06 07:00:13.557054+00:00, run_end_date=2025-09-06 07:00:13.638299+00:00, run_duration=0.081245, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1849, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:00:12.276131+00:00, queued_by_job_id=1732, pid=18227[0m
[[34m2025-09-06T02:00:14.180-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 06:55:00+00:00: scheduled__2025-09-06T06:55:00+00:00, state:running, queued_at: 2025-09-06 07:00:01.579238+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T02:00:14.181-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 06:55:00+00:00, run_id=scheduled__2025-09-06T06:55:00+00:00, run_start_date=2025-09-06 07:00:01.600541+00:00, run_end_date=2025-09-06 07:00:14.181112+00:00, run_duration=12.580571, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 06:55:00+00:00, data_interval_end=2025-09-06 07:00:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T02:00:14.183-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 07:00:00+00:00, run_after=2025-09-06 07:05:00+00:00[0m
[[34m2025-09-06T02:00:58.913-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T02:05:01.533-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 07:05:00+00:00, run_after=2025-09-06 07:10:00+00:00[0m
[[34m2025-09-06T02:05:01.564-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T07:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:05:01.564-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:05:01.564-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T07:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:05:01.565-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T02:05:01.565-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:05:01.569-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:05:02.186-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:05:02.533-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:02.574-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:02.656-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:02.683-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:02.742-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:05:02.743-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:02.767-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T07:00:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:05:03.265-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:05:03.268-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T07:00:00+00:00, map_index=-1, run_start_date=2025-09-06 07:05:02.798958+00:00, run_end_date=2025-09-06 07:05:02.880466+00:00, run_duration=0.081508, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1850, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:05:01.564718+00:00, queued_by_job_id=1732, pid=19433[0m
[[34m2025-09-06T02:05:03.425-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T07:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:05:03.425-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:05:03.425-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T07:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:05:03.426-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T02:05:03.426-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:05:03.430-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:05:04.093-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:05:04.449-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:04.490-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:04.572-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:04.598-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:04.659-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:05:04.660-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:04.684-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T07:00:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:05:05.195-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:05:05.197-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T07:00:00+00:00, map_index=-1, run_start_date=2025-09-06 07:05:04.715527+00:00, run_end_date=2025-09-06 07:05:04.802396+00:00, run_duration=0.086869, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1851, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:05:03.425983+00:00, queued_by_job_id=1732, pid=19444[0m
[[34m2025-09-06T02:05:05.350-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T02:05:05.351-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:05:05.351-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T02:05:05.351-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T02:05:05.351-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T02:05:05.352-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:05:05.352-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T02:05:05.352-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:05:05.352-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T02:05:05.352-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:05:05.352-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T02:05:05.356-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T02:05:05.996-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:05:06.417-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:06.463-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:06.555-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:06.583-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:06.645-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:05:06.645-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:06.663-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:05:07.448-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T02:05:08.074-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:05:08.425-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:08.466-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:08.547-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:08.574-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:08.635-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:05:08.635-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:08.652-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:05:09.342-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T02:05:10.086-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:05:10.486-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:10.535-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:10.631-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:10.663-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:10.741-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:05:10.741-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:10.762-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:00:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:05:11.592-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T02:05:11.592-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T02:05:11.592-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T02:05:11.595-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T07:00:00+00:00, map_index=0, run_start_date=2025-09-06 07:05:06.700039+00:00, run_end_date=2025-09-06 07:05:07.047826+00:00, run_duration=0.347787, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1852, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:05:05.351611+00:00, queued_by_job_id=1732, pid=19455[0m
[[34m2025-09-06T02:05:11.595-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T07:00:00+00:00, map_index=1, run_start_date=2025-09-06 07:05:08.690641+00:00, run_end_date=2025-09-06 07:05:08.949201+00:00, run_duration=0.25856, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1853, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:05:05.351611+00:00, queued_by_job_id=1732, pid=19469[0m
[[34m2025-09-06T02:05:11.595-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T07:00:00+00:00, map_index=2, run_start_date=2025-09-06 07:05:10.806997+00:00, run_end_date=2025-09-06 07:05:11.152955+00:00, run_duration=0.345958, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1854, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:05:05.351611+00:00, queued_by_job_id=1732, pid=19494[0m
[[34m2025-09-06T02:05:11.748-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T07:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:05:11.748-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:05:11.748-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T07:00:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:05:11.749-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T02:05:11.749-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:05:11.752-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:05:12.436-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:05:12.808-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:12.853-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:12.942-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:12.972-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:05:13.039-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:05:13.040-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:05:13.064-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T07:00:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:05:13.590-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T07:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:05:13.592-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T07:00:00+00:00, map_index=-1, run_start_date=2025-09-06 07:05:13.103852+00:00, run_end_date=2025-09-06 07:05:13.191704+00:00, run_duration=0.087852, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1855, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:05:11.749111+00:00, queued_by_job_id=1732, pid=19587[0m
[[34m2025-09-06T02:05:13.727-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 07:00:00+00:00: scheduled__2025-09-06T07:00:00+00:00, state:running, queued_at: 2025-09-06 07:05:01.530836+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T02:05:13.728-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 07:00:00+00:00, run_id=scheduled__2025-09-06T07:00:00+00:00, run_start_date=2025-09-06 07:05:01.544511+00:00, run_end_date=2025-09-06 07:05:13.728005+00:00, run_duration=12.183494, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 07:00:00+00:00, data_interval_end=2025-09-06 07:05:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T02:05:13.730-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 07:05:00+00:00, run_after=2025-09-06 07:10:00+00:00[0m
[[34m2025-09-06T02:05:59.053-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-09-06T02:10:01.372-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 07:10:00+00:00, run_after=2025-09-06 07:15:00+00:00[0m
[[34m2025-09-06T02:10:01.403-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T07:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:10:01.403-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:10:01.403-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T07:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:10:01.404-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-09-06T02:10:01.404-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:10:01.407-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ensure_db', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:10:02.149-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:10:02.508-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:02.551-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:02.635-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:02.663-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:02.727-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:10:02.727-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:02.755-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ensure_db scheduled__2025-09-06T07:05:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:10:03.309-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ensure_db', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:10:03.312-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ensure_db, run_id=scheduled__2025-09-06T07:05:00+00:00, map_index=-1, run_start_date=2025-09-06 07:10:02.795813+00:00, run_end_date=2025-09-06 07:10:02.887888+00:00, run_duration=0.092075, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1856, pool=default_pool, queue=default, priority_weight=4, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:10:01.403911+00:00, queued_by_job_id=1732, pid=20255[0m
[[34m2025-09-06T02:10:03.476-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T07:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:10:03.476-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:10:03.476-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T07:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:10:03.477-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-09-06T02:10:03.477-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:10:03.481-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'list_tickers', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:10:04.130-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:10:04.555-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:04.607-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:04.699-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:04.731-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:04.799-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:10:04.799-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:04.823-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.list_tickers scheduled__2025-09-06T07:05:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:10:05.365-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='list_tickers', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:10:05.367-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=list_tickers, run_id=scheduled__2025-09-06T07:05:00+00:00, map_index=-1, run_start_date=2025-09-06 07:10:04.862658+00:00, run_end_date=2025-09-06 07:10:04.952591+00:00, run_duration=0.089933, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1857, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:10:03.477128+00:00, queued_by_job_id=1732, pid=20266[0m
[[34m2025-09-06T02:10:05.518-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T02:10:05.518-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:10:05.518-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 1/16 running and queued tasks[0m
[[34m2025-09-06T02:10:05.518-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 2/16 running and queued tasks[0m
[[34m2025-09-06T02:10:05.518-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=0 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=1 [scheduled]>
	<TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=2 [scheduled]>[0m
[[34m2025-09-06T02:10:05.519-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:10:05.519-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T02:10:05.519-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:10:05.519-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T02:10:05.519-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2025-09-06T02:10:05.519-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T02:10:05.524-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '0'][0m
[[34m2025-09-06T02:10:06.294-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:10:06.681-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:06.730-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:06.821-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:06.851-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:06.916-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:10:06.917-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:06.935-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=0 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:10:07.786-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '1'][0m
[[34m2025-09-06T02:10:08.565-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:10:08.952-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:09.004-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:09.100-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:09.132-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:09.202-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:10:09.202-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:09.224-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=1 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:10:10.001-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'ingest_one', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py', '--map-index', '2'][0m
[[34m2025-09-06T02:10:10.822-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:10:11.191-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:11.234-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:11.320-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:11.348-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:11.412-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:10:11.412-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:11.430-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.ingest_one scheduled__2025-09-06T07:05:00+00:00 map_index=2 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:10:12.232-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=0)[0m
[[34m2025-09-06T02:10:12.232-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=1)[0m
[[34m2025-09-06T02:10:12.232-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='ingest_one', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=2)[0m
[[34m2025-09-06T02:10:12.235-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T07:05:00+00:00, map_index=0, run_start_date=2025-09-06 07:10:06.979104+00:00, run_end_date=2025-09-06 07:10:07.282181+00:00, run_duration=0.303077, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1858, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:10:05.518738+00:00, queued_by_job_id=1732, pid=20278[0m
[[34m2025-09-06T02:10:12.235-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T07:05:00+00:00, map_index=1, run_start_date=2025-09-06 07:10:09.269623+00:00, run_end_date=2025-09-06 07:10:09.545937+00:00, run_duration=0.276314, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1859, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:10:05.518738+00:00, queued_by_job_id=1732, pid=20294[0m
[[34m2025-09-06T02:10:12.235-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=ingest_one, run_id=scheduled__2025-09-06T07:05:00+00:00, map_index=2, run_start_date=2025-09-06 07:10:11.472308+00:00, run_end_date=2025-09-06 07:10:11.800331+00:00, run_duration=0.328023, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1860, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:10:05.518738+00:00, queued_by_job_id=1732, pid=20311[0m
[[34m2025-09-06T02:10:12.402-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T07:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:10:12.402-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG market_data_pipeline has 0/16 running and queued tasks[0m
[[34m2025-09-06T02:10:12.402-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T07:05:00+00:00 [scheduled]>[0m
[[34m2025-09-06T02:10:12.403-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-09-06T02:10:12.403-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:10:12.408-0500[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'market_data_pipeline', 'show_counts', 'scheduled__2025-09-06T07:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/market_data_pipeline.py'][0m
[[34m2025-09-06T02:10:13.058-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/chethan-kailashnath/airflow/dags/market_data_pipeline.py[0m
[[34m2025-09-06T02:10:13.417-0500[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:13.460-0500[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:13.552-0500[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:13.589-0500[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-09-06T02:10:13.665-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/chethan-kailashnath/.local/lib/python3.12/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-09-06T02:10:13.665-0500[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-09-06T02:10:13.690-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: market_data_pipeline.show_counts scheduled__2025-09-06T07:05:00+00:00 [queued]> on host CK-x1carbon[0m
[[34m2025-09-06T02:10:14.232-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='market_data_pipeline', task_id='show_counts', run_id='scheduled__2025-09-06T07:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-09-06T02:10:14.235-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=market_data_pipeline, task_id=show_counts, run_id=scheduled__2025-09-06T07:05:00+00:00, map_index=-1, run_start_date=2025-09-06 07:10:13.728007+00:00, run_end_date=2025-09-06 07:10:13.810876+00:00, run_duration=0.082869, state=success, executor_state=success, try_number=1, max_tries=1, job_id=1861, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2025-09-06 07:10:12.402935+00:00, queued_by_job_id=1732, pid=20325[0m
[[34m2025-09-06T02:10:14.360-0500[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun market_data_pipeline @ 2025-09-06 07:05:00+00:00: scheduled__2025-09-06T07:05:00+00:00, state:running, queued_at: 2025-09-06 07:10:01.364603+00:00. externally triggered: False> successful[0m
[[34m2025-09-06T02:10:14.360-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=market_data_pipeline, execution_date=2025-09-06 07:05:00+00:00, run_id=scheduled__2025-09-06T07:05:00+00:00, run_start_date=2025-09-06 07:10:01.384716+00:00, run_end_date=2025-09-06 07:10:14.360672+00:00, run_duration=12.975956, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2025-09-06 07:05:00+00:00, data_interval_end=2025-09-06 07:10:00+00:00, dag_hash=18d9c6c4cf02f497ac001a87f69dfa11[0m
[[34m2025-09-06T02:10:14.363-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for market_data_pipeline to 2025-09-06 07:10:00+00:00, run_after=2025-09-06 07:15:00+00:00[0m
[[34m2025-09-06T02:10:59.206-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
